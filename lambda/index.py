# lambda/index.py
import json
import os
import boto3
import re  # æ­£è¦è¡¨ç¾ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from botocore.exceptions import ClientError

#è¿½åŠ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import torch
from transformers import pipeline
import time
import traceback
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
import nest_asyncio
from pyngrok import ngrok



#è¿½åŠ ã‚³ãƒ¼ãƒ‰~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

app = FastAPI()

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å½¢å¼ã‚’å®šç¾©
class InputData(BaseModel):
    message: str

# ç›´æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆ
class SimpleGenerationRequest(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 512
    do_sample: Optional[bool] = True
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

class GenerationResponse(BaseModel):
    generated_text: str
    response_time: float

model = None

MODEL_ID = os.environ.get("MODEL_ID", "us.amazon.nova-lite-v1:0")

def load_model():
    """æ¨è«–ç”¨ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    global model  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã«å¿…è¦
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
        pipe = pipeline(
            "text-generation",
            model=config.MODEL_ID,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device
        )
        print(f"ãƒ¢ãƒ‡ãƒ« '{config.MODEL_ID}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸ")
        model = pipe  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
        return pipe
    except Exception as e:
        error_msg = f"ãƒ¢ãƒ‡ãƒ« '{config.MODEL_ID}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}"
        print(error_msg)
        traceback.print_exc()  # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
        return None

def load_model_task():
    """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯"""
    global model
    print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")
    # load_modelé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«è¨­å®š
    loaded_pipe = load_model()
    if loaded_pipe:
        model = loaded_pipe  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
        print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    else:
        print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

print("FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å®šç¾©ã—ã¾ã—ãŸã€‚")

# --- FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---
@app.on_event("startup")
async def startup_event():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
    load_model_task()  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã¯ãªãåŒæœŸçš„ã«èª­ã¿è¾¼ã‚€
    if model is None:
        print("è­¦å‘Š: èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        print("èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

@app.get("/")
async def root():
    """åŸºæœ¬çš„ãªAPIãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {"status": "ok", "message": "Local LLM API is runnning"}

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    global model
    if model is None:
        return {"status": "error", "message": "No model loaded"}

    return {"status": "ok", "model": config.MODEL_ID}

# ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/generate", response_model=GenerationResponse)
async def generate_simple(request: SimpleGenerationRequest):
    """å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    global model

    if model is None:
        print("generateã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™...")
        load_model_task()  # å†åº¦èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã‚‹
        if model is None:
            print("generateã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å¾Œã§ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

    try:
        start_time = time.time()
        print(f"ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡: prompt={request.prompt[:100]}..., max_new_tokens={request.max_new_tokens}")  # é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯åˆ‡ã‚Šæ¨ã¦

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ç›´æ¥å¿œç­”ã‚’ç”Ÿæˆ
        print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’é–‹å§‹...")
        outputs = model(
            request.prompt,
            max_new_tokens=request.max_new_tokens,
            do_sample=request.do_sample,
            temperature=request.temperature,
            top_p=request.top_p,
        )
        print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’æŠ½å‡º
        assistant_response = extract_assistant_response(outputs, request.prompt)
        print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”: {assistant_response[:100]}...")  # é•·ã„å ´åˆã¯åˆ‡ã‚Šæ¨ã¦

        end_time = time.time()
        response_time = end_time - start_time
        print(f"å¿œç­”ç”Ÿæˆæ™‚é–“: {response_time:.2f}ç§’")

        return GenerationResponse(
            generated_text=assistant_response,
            response_time=response_time
        )

    except Exception as e:
        print(f"ã‚·ãƒ³ãƒ—ãƒ«å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


class Config:
    def __init__(self, model_name=MODEL_ID):
        self.MODEL_ID = model_name

config = Config(MODEL_ID)



#å…¬é–‹urlã®ä½œæˆ
def run_with_ngrok(port=8501):
    """ngrokã§FastAPIã‚¢ãƒ—ãƒªã‚’å®Ÿè¡Œ"""
    nest_asyncio.apply()

    ngrok_token = os.environ.get("NGROK_TOKEN")
    if not ngrok_token:
        print("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒ'NGROK_TOKEN'ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        try:
            print("Colab Secrets(å·¦å´ã®éµã‚¢ã‚¤ã‚³ãƒ³)ã§'NGROK_TOKEN'ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
            ngrok_token = input("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (https://dashboard.ngrok.com/get-started/your-authtoken): ")
        except EOFError:
            print("\nã‚¨ãƒ©ãƒ¼: å¯¾è©±å‹å…¥åŠ›ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            print("Colab Secretsã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚»ãƒ«ã§`os.environ['NGROK_TOKEN'] = 'ã‚ãªãŸã®ãƒˆãƒ¼ã‚¯ãƒ³'`ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            return

    if not ngrok_token:
        print("ã‚¨ãƒ©ãƒ¼: Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¸­æ­¢ã—ã¾ã™ã€‚")
        return

    try:
        ngrok.set_auth_token(ngrok_token)

        # æ—¢å­˜ã®ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚‹
        try:
            tunnels = ngrok.get_tunnels()
            if tunnels:
                print(f"{len(tunnels)}å€‹ã®æ—¢å­˜ãƒˆãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚é–‰ã˜ã¦ã„ã¾ã™...")
                for tunnel in tunnels:
                    print(f"  - åˆ‡æ–­ä¸­: {tunnel.public_url}")
                    ngrok.disconnect(tunnel.public_url)
                print("ã™ã¹ã¦ã®æ—¢å­˜ngrokãƒˆãƒ³ãƒãƒ«ã‚’åˆ‡æ–­ã—ã¾ã—ãŸã€‚")
            else:
                print("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªngrokãƒˆãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"ãƒˆãƒ³ãƒãƒ«åˆ‡æ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšç¶šè¡Œã‚’è©¦ã¿ã‚‹

        # æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã
        print(f"ãƒãƒ¼ãƒˆ{port}ã«æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã„ã¦ã„ã¾ã™...")
        ngrok_tunnel = ngrok.connect(port)
        public_url = ngrok_tunnel.public_url
        print("---------------------------------------------------------------------")
        print(f"âœ… å…¬é–‹URL:   {public_url}")
        print(f"ğŸ“– APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (Swagger UI): {public_url}/docs")
        print("---------------------------------------------------------------------")
        print("(APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚„ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ã“ã®URLã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„)")
        uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’infoã«è¨­å®š

    except Exception as e:
        print(f"\n ngrokã¾ãŸã¯Uvicornã®èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚ˆã†ã¨ã™ã‚‹
        try:
            print("ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¦ã„ã¾ã™...")
            tunnels = ngrok.get_tunnels()
            for tunnel in tunnels:
                ngrok.disconnect(tunnel.public_url)
            print("ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¾ã—ãŸã€‚")
        except Exception as ne:
            print(f"ngrokãƒˆãƒ³ãƒãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«åˆ¥ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {ne}")
run_with_ngrok(port=8501)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Lambda ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
def extract_region_from_arn(arn):
    # ARN å½¢å¼: arn:aws:lambda:region:account-id:function:function-name
    match = re.search('arn:aws:lambda:([^:]+):', arn)
    if match:
        return match.group(1)
    return "us-east-1"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆåˆæœŸå€¤ï¼‰
bedrock_client = None

# ãƒ¢ãƒ‡ãƒ«ID
MODEL_ID = os.environ.get("MODEL_ID", "us.amazon.nova-lite-v1:0")

def lambda_handler(event, context):
    try:
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å®Ÿè¡Œãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        global bedrock_client
        if bedrock_client is None:
            region = extract_region_from_arn(context.invoked_function_arn)
            bedrock_client = boto3.client('bedrock-runtime', region_name=region)
            print(f"Initialized Bedrock client in region: {region}")
        
        print("Received event:", json.dumps(event))
        
        # Cognitoã§èªè¨¼ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
        user_info = None
        if 'requestContext' in event and 'authorizer' in event['requestContext']:
            user_info = event['requestContext']['authorizer']['claims']
            print(f"Authenticated user: {user_info.get('email') or user_info.get('cognito:username')}")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®è§£æ
        body = json.loads(event['body'])
        message = body['message']
        conversation_history = body.get('conversationHistory', [])
        
        print("Processing message:", message)
        print("Using model:", MODEL_ID)
        
        # ä¼šè©±å±¥æ­´ã‚’ä½¿ç”¨
        messages = conversation_history.copy()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        messages.append({
            "role": "user",
            "content": message
        })
        
        # Nova Liteãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’æ§‹ç¯‰
        # ä¼šè©±å±¥æ­´ã‚’å«ã‚ã‚‹
        bedrock_messages = []
        for msg in messages:
            if msg["role"] == "user":
                bedrock_messages.append({
                    "role": "user",
                    "content": [{"text": msg["content"]}]
                })
            elif msg["role"] == "assistant":
                bedrock_messages.append({
                    "role": "assistant", 
                    "content": [{"text": msg["content"]}]
                })
        
        # invoke_modelç”¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
        request_payload = {
            "messages": bedrock_messages,
            "inferenceConfig": {
                "maxTokens": 512,
                "stopSequences": [],
                "temperature": 0.7,
                "topP": 0.9
            }
        }
        
        print("Calling Bedrock invoke_model API with payload:", json.dumps(request_payload))
        
        # invoke_model APIã‚’å‘¼ã³å‡ºã—
        response = bedrock_client.invoke_model(
            modelId=MODEL_ID,
            body=json.dumps(request_payload),
            contentType="application/json"
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
        response_body = json.loads(response['body'].read())
        print("Bedrock response:", json.dumps(response_body, default=str))
        
        # å¿œç­”ã®æ¤œè¨¼
        if not response_body.get('output') or not response_body['output'].get('message') or not response_body['output']['message'].get('content'):
            raise Exception("No response content from the model")
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’å–å¾—
        assistant_response = response_body['output']['message']['content'][0]['text']
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        messages.append({
            "role": "assistant",
            "content": assistant_response
        })
        
        # æˆåŠŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¿”å´
        return {
            "statusCode": 200,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token",
                "Access-Control-Allow-Methods": "OPTIONS,POST"
            },
            "body": json.dumps({
                "success": True,
                "response": assistant_response,
                "conversationHistory": messages
            })
        }
        
    except Exception as error:
        print("Error:", str(error))
        
        return {
            "statusCode": 500,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token",
                "Access-Control-Allow-Methods": "OPTIONS,POST"
            },
            "body": json.dumps({
                "success": False,
                "error": str(error)
            })
        }
